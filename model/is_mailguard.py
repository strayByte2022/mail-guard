# -*- coding: utf-8 -*-
from ml_utils import clean_texts
"""IS_Mailguard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RQFEK7xmzH0vluVMwGWioVdMwxTz7YQ6

# Data Preprocessing

The dataset used in this project is the "Emails for Spam or Ham Classification" dataset, available on Kaggle. It contains a balanced and pre-labeled collection of spam and non-spam (ham) email messages, primarily sourced from the well-known SpamAssassin public corpus. The dataset is curated by bayes2003 and serves as a reliable benchmark for email classification tasks. https://www.kaggle.com/datasets/bayes2003/emails-for-spam-or-ham-classification-spamassassin
"""

#!pip install --quiet scikit-learn imbalanced-learn nltk

import pandas as pd, textwrap, itertools, collections

df = pd.read_csv("email_text.csv")

print("Shape :", df.shape)
print("Columns:", list(df.columns))

# pd.options.display.max_colwidth = 120
# display(df.head())

"""## Clean up dataframe"""

# Drop missing / empty rows
df = df.dropna(subset=["text"])
df["text"] = df["text"].astype(str).str.strip()
df = df[df["text"] != ""]

# Normalise label to 0/1
label_map = {"spam": 1, "ham": 0, "Spam": 1, "Ham": 0, 1:1, 0:0}
df["label"] = df["label"].map(label_map).astype(int)

print("Rows after cleaning:", df.shape[0])
print(df["label"].value_counts())

"""## Text cleaner"""

import re, nltk, string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download("stopwords", quiet=True)

stop_words = set(stopwords.words("english"))
stemmer    = PorterStemmer()

def clean_email(s: str) -> str:
    s = s.lower()
    s = re.sub(r"<[^>]+>", " ", s)          # drop HTML tags
    s = re.sub(r"[^a-z0-9\s]", " ", s)      # keep alphanumerics
    toks = [w for w in s.split() if len(w) > 2]
    toks = [stemmer.stem(w) for w in toks if w not in stop_words]
    return " ".join(toks)

from sklearn.preprocessing import FunctionTransformer
clean_fn = FunctionTransformer(lambda X: [clean_email(t) for t in X],
                               validate=False)   # keep sparse

# """## Figure 1 – Class balance"""

# import matplotlib.pyplot as plt, seaborn as sns, pandas as pd
# sns.countplot(x=df["label"].map({0:"HAM",1:"SPAM"}))
# plt.title("Label Distribution"); plt.xlabel(""); plt.ylabel("Count")
# plt.tight_layout(); plt.savefig("label_dist.png")

# """## Figure 2 – Message-length histogram"""

# msg_len = df["text"].str.split().str.len()
# plt.hist(msg_len, bins=40); plt.title("Token Count per Email")
# plt.xlabel("tokens"); plt.ylabel("emails"); plt.tight_layout()
# plt.savefig("length_hist.png")

# """# Stratified 80 / 10 / 10 split"""

from sklearn.model_selection import train_test_split
X_train, X_temp, y_train, y_temp = train_test_split(
    df.text, df.label, test_size=0.2, stratify=df.label, random_state=42)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

print("Splits:", len(X_train), len(X_val), len(X_test))

"""# FeatureUnion"""

from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

word_tfidf = TfidfVectorizer(
    analyzer="word",
    ngram_range=(1,2),
    min_df=3,
    max_df=0.90,
    max_features=7_500
)

char_tfidf = TfidfVectorizer(
    analyzer="char_wb",
    ngram_range=(3,5),
    min_df=1,
    max_df=0.90,
    max_features=3_000
)

vectoriser = FeatureUnion(
    [("word_ngram", word_tfidf),
     ("char_ngram", char_tfidf)]
)

"""# Full pipeline"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ("clean",    clean_fn),
    ("features", vectoriser),
    ("clf",      LogisticRegression(
                    max_iter=2000,
                    solver="saga",
                    class_weight="balanced",))
])

"""# Grid"""

from sklearn.model_selection import GridSearchCV, StratifiedKFold

param_grid = {
    "features__word_ngram__max_features": [5_000, 7_500],

    "features__word_ngram__ngram_range": [(1,2)],

    "clf__C": [16, 32]
}

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

grid = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    scoring={"F1": "f1", "AUC": "roc_auc"},
    refit="F1",
    cv=cv,
    n_jobs=1,
    verbose=3,
    error_score="raise"
)

grid.fit(X_train, y_train)
print("Best F1:", grid.best_score_)
print("Best params:", grid.best_params_)
best_model = grid.best_estimator_

"""# Evaluation"""

from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import RocCurveDisplay

y_pred  = best_model.predict(X_test)
y_prob  = best_model.predict_proba(X_test)[:,1]

print(classification_report(y_test, y_pred, digits=4))

print("ROC-AUC:", roc_auc_score(y_test, y_prob))
RocCurveDisplay.from_predictions(y_test, y_prob)
#plt.title("ROC Curve – Test Set"); plt.tight_layout()
#plt.savefig("roc_curve.png")

print("Confusion:\n", confusion_matrix(y_test, y_pred))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap="Blues")
#plt.title("Confusion Matrix – Test Set"); plt.tight_layout()
#plt.savefig("conf_mat.png")



best_model.named_steps["clean"].func = clean_texts

import joblib, os
joblib.dump(best_model, "spam_pipeline.pkl")
print("Saved →", os.path.getsize("spam_pipeline.pkl")/1024, "KB")